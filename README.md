# **ELT Pipeline with PostgreSQL and Docker**

## **ğŸ“Œ Overview**
This ELT (Extract, Load, Transform) pipeline automates data transfer between two PostgreSQL databases using **Docker Compose**. It extracts data from `source_postgres`, saves it to an SQL file (`data_dump.sql`), and loads it into `destination_postgres` using PostgreSQL command-line tools.

---

## **ğŸ“‚ Project Structure**
```
elt/
â”‚-- source_db_init/
â”‚   â”œâ”€â”€ init.sql          # Pre-populates `source_postgres` with tables and data
â”‚-- elt_script.py         # Python script to handle ELT process
â”‚-- Dockerfile            # Builds the ELT container
â”‚-- docker-compose.yml    # Defines the services (PostgreSQL & ELT script)
â”‚-- README.md             # Documentation
```

---

## **âš¡ Technologies Used**
- **PostgreSQL 17.4**
- **Docker & Docker Compose**
- **Python 3.8**
- **PostgreSQL CLI Tools (`pg_dump`, `psql`)**

---

## **ğŸš€ Getting Started**

### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/I-Akash-Sharma/elt/
cd elt
```

### **2ï¸âƒ£ Build and Start the Containers**
```bash
docker compose up -d --build
```
This will:
- Start `source_postgres` (Port **5433**)
- Start `destination_postgres` (Port **5434**)
- Run `elt_script.py` to migrate data

### **3ï¸âƒ£ Verify Running Containers**
```bash
docker ps
```
You should see:
```
source_postgres   Up (Port 5433)
destination_postgres   Up (Port 5434)
elt_script   Exited (successful completion)
```

### **4ï¸âƒ£ Check Data in Destination Database**
Connect to `destination_postgres`:
```bash
psql -h localhost -p 5434 -U postgres -d destination_db
```
Run:
```sql
SELECT * FROM users;
SELECT * FROM films;
```
âœ… You should see data transferred from `source_db`!

---

## **ğŸ›  How It Works**
### **1ï¸âƒ£ Database Initialization (`source_db_init/init.sql`)**
- Creates **tables** (`users`, `films`, `film_category`, `actors`, `film_actors`)
- Inserts **sample data** into `source_postgres`

### **2ï¸âƒ£ ELT Script (`elt_script.py`)**
- **Waits for `source_postgres` to be ready** using `pg_isready`
- **Dumps `source_db` into `data_dump.sql`** using `pg_dump`
- **Loads the dump into `destination_db`** using `psql`

### **3ï¸âƒ£ Docker Compose (`docker-compose.yml`)**
- Defines **two PostgreSQL services** (`source_postgres`, `destination_postgres`)
- Runs the **ELT script in a Python container**
- Connects all services via **Docker Network (`elt_network`)**

---

## **ğŸ“„ Configuration Details**
### **Environment Variables (Set in `docker-compose.yml`)**
| Variable         | `source_postgres` | `destination_postgres` |
|-----------------|------------------|-----------------------|
| `POSTGRES_DB`   | source_db        | destination_db       |
| `POSTGRES_USER` | postgres         | postgres             |
| `POSTGRES_PASSWORD` | secret       | secret               |

---

## **ğŸ“Œ Useful Commands**
### **ğŸ”„ Restart the Pipeline**
```bash
docker compose down --volumes
```
```bash
docker compose up -d --build
```

### **ğŸ“œ Check Logs**
```bash
docker logs <container_id>
```

### **ğŸ›‘ Stop and Remove Containers**
```bash
docker compose down
```

---

## **ğŸ¯ Future Improvements**
âœ… Implement **data transformations** before loading into `destination_postgres`
âœ… Use **environment variables** for database credentials
âœ… Add **logging and monitoring** for ELT process

---

## **ğŸ“¢ Support**
For any issues, feel free to open a GitHub **issue** or reach out! ğŸš€

